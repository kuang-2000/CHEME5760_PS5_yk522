{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74c7da3d-5bac-4d7d-81c2-f135ae11d8cc",
   "metadata": {},
   "source": [
    "## PS5: Markov Decision Processes and Value Iteration\n",
    "\n",
    "### Markov Decision Processes (MDPs)\n",
    "In an MDP, an agent receives a reward or penalty for each decision. MDPs consist of the tuple $\\left(\\mathcal{S}, \\mathcal{A}, R_{a}\\left(s, s^{\\prime}\\right), T_{a}\\left(s,s^{\\prime}\\right), \\gamma\\right)$:\n",
    "\n",
    "* The state space $\\mathcal{S}$ is the set of all possible states $s$ that a system can exist in.\n",
    "* The action space $\\mathcal{A}$ is the set of all possible actions $a$ that are available to the agent, where $\\mathcal{A}_{s} \\subseteq \\mathcal{A}$ is the subset of the action space $\\mathcal{A}$ that is accessible from state $s$.\n",
    "* An reward $R_{a}\\left(s, s^{\\prime}\\right)$ is received after transitioning from $s\\rightarrow{s}^{\\prime}$ due to action $a$. \n",
    "* The state transition model $T_{a}\\left(s,s^{\\prime}\\right) = P(s_{t+1} = s^{\\prime}~|~s_{t}=s,a_{t} = a)$ denotes the probability that action $a$ in state $s$ at time $t$ will result in state $s^{\\prime}$ at time $t+1$\n",
    "* The quantity $\\gamma$ is a discount factor used to weigh the future expected utility.\n",
    "\n",
    "Finally, a policy function $\\pi$ is the mapping from states $s\\in\\mathcal{S}$ to actions $a\\in\\mathcal{A}$ used by the agent to solve a decision task, i.e., $\\pi(s) = a$.\n",
    "\n",
    "### Value Iteration\n",
    "_Value iteration_ iteratively computes the optimal value function $U^{*}(s)$ using the _Bellman backup_ operation:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "U_{k+1}(s) = \\underset{a\\in\\mathcal{A}}{\\max}\\left(R(s,a) + \\gamma\\cdot\\sum_{s^{\\prime}\\in\\mathcal{S}}T(s^{\\prime}\\,\\vert\\,s,a)\\cdot{U}_{k}(s^{\\prime})\\right)\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "As $k\\rightarrow\\infty$ the value function $U_{k}(s)$ converges to the optimal value function $U^{\\star}(s)$. The optimal policy $\\pi^{\\star}(s)$ can be extracted from the $Q(s,a)$-function:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "Q^{\\star}(s,a) = R(s,a) + \\gamma\\times{\\text{sum}([T(s,s^{\\prime},a)\\times{U^{\\star}}(s^{\\prime})\\,\\,\\text{for}\\,s^{\\prime} \\in\\mathcal{S}])}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "by selecting the action $a$ such that:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\pi^{\\star}(s) = \\underset{a\\in\\mathcal{A}}{\\arg\\max}\\,Q^{\\star}(s,a)\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "### Problem and learning objectives\n",
    "You have a [roomba](https://www.irobot.com) that has finished cleaning the kitchen floor and needs to return to its charging station. However, between your kitchen floor and the `charging station` (safety), there are one or more `lava pits` (destruction for the [roomba](https://www.irobot.com)). This is an example of a two-dimensional grid-world navigational decision task. \n",
    "\n",
    "`PS5` will familiarize students with using `value iteration` for solving a two-dimensional grid-world navigation task, the role of the discount factor $\\gamma$, and the number of iterations $k_{\\text{max}}$. In particular, we will:\n",
    "\n",
    "* __Task 1__: Setup a $10\\times{10}$ grid, encoded this model as an instance of the `MyRectangularGridWorldModel` type\n",
    "    * `TODO`: Inspect the data inside our grid world model and understand what each describes (similar to `Lab 9c`, just bigger).\n",
    "    * `TODO`: Add `lava pits` at $(5,5)$, $(6,5)$ and $(6,6)$.\n",
    "    * `TODO`: Add a `charging station` at $(5,6)$.\n",
    "* __Task 2__: Use our `MyRectangularGridWorldModel` instance and generate the components of the `MDP`, namely, the return function (or array) `R(s, a)`, and the model of the physics of the world in the transition function (or array) `T(s, s′, a)`.\n",
    "    * `TODO`: Modify the $R(s, a)$ function so that it describes a `soft wall` at $(2,2)$ and $(2,3)$, i.e., a region where the [roomba](https://www.irobot.com) is _discouraged_ from going. Set the wall penalty as `-1000`. \n",
    "* __Task 3__: For $\\gamma = 0.1,0.5,0.95\\times{k}_{\\max}=10,100,1000$ use `value iteration` to estimate the optimal value function $U^{\\star}(s)$. \n",
    "    * `TODO`: For each combination of ($\\gamma$,$k_{\\max}$), extract the `action-value function` or $Q(s, a)$ from the optimal optimal value function $U^{\\star}(s)$, and compute the optimal navigation policy $\\pi^{\\star}(s)$ from $Q(s,a)$\n",
    "        * Store the policy $\\pi^{\\star}(s)$ in a `Dictionary` where the `keys` are the combinations of ($\\gamma$,$k_{\\max}$) and the `values` are the optimal policy $\\pi^{\\star}(s)$.\n",
    "    * `TODO`: Develop an approach to visualize an optimal policy and explore the policies for different parameter combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b91c6d-e13e-4b8c-bf14-d4651ef03d5e",
   "metadata": {},
   "source": [
    "## Setup\n",
    "The computations in this lab (or example) are enabled by the [VLDecisionsPackage.jl](https://github.com/varnerlab/VLDecisionsPackage.jl.git) and several external `Julia` packages. To load the required packages and any custom codes the teaching team has developed to work with these packages, we [include](https://docs.julialang.org/en/v1/manual/code-loading/) the `Include.jl` file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8de6de82-32d0-4ea2-93b5-6c55c94d2e8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m git-repo `https://github.com/varnerlab/VLDecisionsPackage.jl.git`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/PS5-CHEME-5760-LavaWorld-Fall-2023/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/PS5-CHEME-5760-LavaWorld-Fall-2023/Manifest.toml`\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m git-repo `https://github.com/varnerlab/VLQuantitativeFinancePackage.jl.git`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/PS5-CHEME-5760-LavaWorld-Fall-2023/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/PS5-CHEME-5760-LavaWorld-Fall-2023/Manifest.toml`\n",
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Desktop/julia_work/PS5-CHEME-5760-LavaWorld-Fall-2023`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/PS5-CHEME-5760-LavaWorld-Fall-2023/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/PS5-CHEME-5760-LavaWorld-Fall-2023/Manifest.toml`\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General.toml`\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m git-repo `https://github.com/varnerlab/VLDecisionsPackage.jl.git`\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m git-repo `https://github.com/varnerlab/VLQuantitativeFinancePackage.jl.git`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/PS5-CHEME-5760-LavaWorld-Fall-2023/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/PS5-CHEME-5760-LavaWorld-Fall-2023/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232d016f-ec6a-4a9d-9353-3ef84eacc4eb",
   "metadata": {},
   "source": [
    "### Types\n",
    "\n",
    "We model the grid world using the `MyRectangularGridWorldModel` type. \n",
    "* We pass the number of rows in the `nrows::Int` field, the number of columns in the `ncols::Int` field, and an approximate representation of the problem rewards in the `rewards::Dict{Tuple{Int,Int}, Float64}` field. The other fields are then populated during the `build(...)` process:\n",
    "\n",
    "```julia\n",
    "mutable struct MyRectangularGridWorldModel <: AbstractWorldModel\n",
    "\n",
    "    # data -\n",
    "    number_of_rows::Int\n",
    "    number_of_cols::Int\n",
    "    coordinates::Dict{Int,Tuple{Int,Int}}\n",
    "    states::Dict{Tuple{Int,Int},Int}\n",
    "    moves::Dict{Int,Tuple{Int,Int}}\n",
    "    rewards::Dict{Int,Float64}\n",
    "\n",
    "    # constructor -\n",
    "    MyRectangularGridWorldModel() = new();\n",
    "end\n",
    "```\n",
    "\n",
    "The `MyMDPProblemModel` type models a Markov Decision Process (MDPs). \n",
    "* We pass the states `𝒮`, the actions `𝒜`, the transition matrix `T`, the reward matrix `R`, and the discount factor `γ` into the `build(...)` method. \n",
    "\n",
    "```julia\n",
    "mutable struct MyMDPProblemModel <: AbstractProcessModel\n",
    "\n",
    "    # data -\n",
    "    𝒮::Array{Int64,1}\n",
    "    𝒜::Array{Int64,1}\n",
    "    T::Union{Function, Array{Float64,3}}\n",
    "    R::Union{Function, Array{Float64,2}}\n",
    "    γ::Float64\n",
    "    \n",
    "    # constructor -\n",
    "    MyMDPProblemModel() = new()\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa90108b-9ca9-4ab9-aba1-5085799c43fd",
   "metadata": {},
   "source": [
    "## Task 1: Build the world model\n",
    "We encoded the `rectangular grid world` using the `MyRectangularGridWorldModel` model, which we construct using a `build(...)` method. Let's setup the data for the world, setup the states, actions, rewards and then construct the world model. \n",
    "* First, set values for the `number_of_rows` and `number_of_cols` variables, the `nactions` that are avialble to the agent and the `discount factor` $\\gamma$. \n",
    "* Then, we'll compute the number of states, and setup the state set $\\mathcal{S}$ and the action set $\\mathcal{A}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f4d2229-9484-4320-ad55-3c597a8b3881",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "number_of_rows = 10\n",
    "number_of_cols = 10\n",
    "nactions = 4;\n",
    "nstates = (number_of_rows*number_of_cols);\n",
    "𝒮 = range(1,stop=nstates,step=1) |> collect;\n",
    "𝒜 = range(1,stop=nactions,step=1) |> collect;\n",
    "γ = 0.95;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ea7b15-294b-485a-8b59-3442fc339f06",
   "metadata": {},
   "source": [
    "Next, we'll set up a description of the rewards, the `rewards::Dict{Tuple{Int,Int}, Float64}` dictionary, which maps the $(x,y)$-coordinates to a reward value. We only need to put `non-default` reward values in the reward dictionary (we'll add default values to the other locations later). Lastly, let's put the locations on the grid that are `absorbing`, meaning the charging station or lava pits in your living room:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "585ea9b1-a9f9-4628-9bb7-f43d7811f970",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "lava_reward = -100000.0;\n",
    "charging_reward = 10000.0;\n",
    "\n",
    "# setup rewards -\n",
    "rewards = Dict{Tuple{Int,Int}, Float64}()\n",
    "rewards[(5,5)] =  lava_reward # lava in the (5,5) square \n",
    "rewards[(6,5)] = lava_reward # lava in the (6,5) square\n",
    "rewards[(6,6)] = lava_reward # lava in the (6,6) square\n",
    "rewards[(5,6)] = charging_reward    # charging station square\n",
    "\n",
    "# setup set of absorbing states -\n",
    "absorbing_state_set = Set{Tuple{Int,Int}}()\n",
    "push!(absorbing_state_set, (5,5));\n",
    "push!(absorbing_state_set, (6,5));\n",
    "push!(absorbing_state_set, (6,6));\n",
    "push!(absorbing_state_set, (5,6));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcf87c2-0b91-458e-8fb0-bb0db4e451ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "Finally, we can build an instance of the `MyRectangularGridWorldModel` type, which models the grid world. We save this instance in the `world` variable\n",
    "* We must pass in the number of rows `nrows`, number of cols `ncols`, and our initial reward description in the `rewards` field into the `build(...)` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80f48d7a-a171-418d-8e99-cedb894ef305",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "world = VLDecisionsPackage.build(MyRectangularGridWorldModel, \n",
    "    (nrows = number_of_rows, ncols = number_of_cols, rewards = rewards));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7af1d83-8feb-47ff-b100-1bc01bfe7ef9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Questions\n",
    "* `TODO`: Inspect the data inside our grid world model, understand what each describes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0ed737-187b-4c88-88e4-dcca9d79e66a",
   "metadata": {},
   "source": [
    "## Task 2: Generate the components of the MDP problem\n",
    "The MDP problem requires the return function (or array) `R(s, a)`, and the transition function (or array) `T(s, s′, a)`. Let's construct these from our grid world model instance, starting with the reward function `R(s, a)`:\n",
    "\n",
    "### Rewards $R(s,a)$\n",
    "We'll encode the reward function as a $\\dim\\mathcal{S}\\times\\dim\\mathcal{A}$ array, which holds the reward values for being in state $s\\in\\mathcal{S}$ and taking action $a\\in\\mathcal{A}$. After initializing the `R`-array and filling it with zeros, we'll populate the non-zero values of $R(s, a)$ using nested `for` loops. During each iteration of the `outer` loop, we'll:\n",
    "* Select a state `s`, an action `a`, and a move `Δ`\n",
    "* We'll then compute the new position resulting from implementing action `a` from the current position and store this in the `new_position` variable. * If the `new_position`$\\in\\mathcal{S}$ is in our initial `rewards` dictionary (the charging station or a lava pit), we use that reward value from the `rewards` dictionary. If we are still in the world but not in a special location, we set the reward to `-1`.\n",
    "* Finally, if `new_position`$\\notin\\mathcal{S}$, i.e., the `new_position` is a space outside the grid, we set a penalty of `-50000.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f51634b4-76cf-4533-81d2-60e1470117dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "R = zeros(nstates, nactions);\n",
    "fill!(R, 0.0)\n",
    "for s ∈ 𝒮\n",
    "    for a ∈ 𝒜\n",
    "        \n",
    "        Δ = world.moves[a];\n",
    "        current_position = world.coordinates[s]\n",
    "        new_position =  current_position .+ Δ\n",
    "        if (haskey(world.states, new_position) == true)\n",
    "            if (haskey(rewards, new_position) == true)\n",
    "                R[s,a] = rewards[new_position];\n",
    "            else\n",
    "                R[s,a] = -1.0;\n",
    "            end\n",
    "        else\n",
    "            R[s,a] = -50000.0; # we are off the grid, big negative penalty\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "soft_wall_set = Set{Tuple{Int,Int}}();\n",
    "push!(soft_wall_set, (2,1));\n",
    "push!(soft_wall_set, (2,2));\n",
    "push!(soft_wall_set, (2,3));\n",
    "push!(soft_wall_set, (7,4));\n",
    "for s ∈ 𝒮\n",
    "    \n",
    "    current_position = world.coordinates[s]\n",
    "    for a ∈ 𝒜\n",
    "        Δ = world.moves[a];\n",
    "        new_position =  current_position .+ Δ\n",
    "        \n",
    "        if (in(new_position, soft_wall_set) == true)\n",
    "          R[s,a] = -1000.0  \n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe1a7ce-5618-45c8-98a8-cdf322c1b172",
   "metadata": {},
   "source": [
    "#### Check: Do your entries of `R` make sense to you?\n",
    "Go through your entries in `R`, and check is the `soft wall` is implemented correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40727829-a6d9-446a-8384-83828d967028",
   "metadata": {},
   "source": [
    "### Transition $T(s, s^{\\prime},a)$\n",
    "Next, build the transition function $T(s,s^{\\prime},a)$. We'll encode this as a $\\dim\\mathcal{S}\\times\\dim\\mathcal{S}\\times\\dim\\mathcal{A}$ [multidimension array](https://docs.julialang.org/en/v1/manual/arrays/) and populate it using nested `for` loops. \n",
    "\n",
    "* The `outer` loop we will iterate over actions. For every $a\\in\\mathcal{A}$ will get the move associated with that action and store it in the `Δ`\n",
    "* In the `inner` loop, we will iterate over states $s\\in\\mathcal{S}$. We compute a `new_position` resulting from implementing action $a$ and check if `new_position`$\\in\\mathcal{S}$. If `new_position` is in the world, and `current_position` is _not_ an `absorbing state` we set $s^{\\prime}\\leftarrow$`world.states[new_position]`, and `T[s, s′,  a] = 1.0`\n",
    "* However, if the `new_position` is outside of the grid (or we are jumping from an `absorbing` state), we set `T[s, s,  a] = 1.0`, i.e., the probability that we stay in `s` if we take action `a` is `1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13d92d6f-fcd5-44be-acc3-58a275d2987e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "T = Array{Float64,3}(undef, nstates, nstates, nactions);\n",
    "fill!(T, 0.0)\n",
    "for a ∈ 𝒜\n",
    "    \n",
    "    Δ = world.moves[a];\n",
    "    \n",
    "    for s ∈ 𝒮\n",
    "        current_position = world.coordinates[s]\n",
    "        new_position =  current_position .+ Δ\n",
    "        if (haskey(world.states, new_position) == true && \n",
    "                in(current_position, absorbing_state_set) == false)\n",
    "            s′ = world.states[new_position];\n",
    "            T[s, s′,  a] = 1.0\n",
    "        else\n",
    "            T[s, s,  a] = 1.0\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4044aa-b380-4a8c-9252-eae71914e1e3",
   "metadata": {},
   "source": [
    "## Task 3: Estimate the optimal value function $U^{\\star}(s)$ for combinations of $(\\gamma,k_{\\max})$\n",
    "For each combination of $\\gamma = 0.1,0.5,0.95\\times{k}_{\\max}=10,100,1000$ use `value iteration` to estimate the optimal value function $U^{\\star}(s)$. \n",
    "* `TODO`: For each combination of ($\\gamma$,$k_{\\max}$), extract the `action-value function` or $Q(s, a)$ from the optimal value function $U^{\\star}(s)$, and compute the optimal navigation policy $\\pi^{\\star}(s)$ from $Q(s,a)$\n",
    "    * Store the optimal policy $\\pi^{\\star}(s)$ in a `Dictionary` where the `keys` are the combinations of ($\\gamma$,$k_{\\max}$) and the `values` are the optimal policy $\\pi^{\\star}(s)$.\n",
    "* `TODO`: Develop an approach to visualize an optimal policy (similar to `Lab 9c` is fine). \n",
    "    * Are the policies different for different parameter combinations, or is our `value iteration` algorithm robust to the parameter choice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "425049e8-8b3e-4f97-9b53-d0ea70fb5541",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Complete this implementation\n",
    "policy_dictionary = Dict{Tuple{Float64,Int64}, Vector{Int64}}()\n",
    "γ_array = [0.1,0.5,0.95];\n",
    "k_array = [1,10,100,1000];\n",
    "for i ∈ eachindex(γ_array)\n",
    "    \n",
    "    γ = γ_array[i];\n",
    "    \n",
    "    # ok, so we have a γ, create a new MDP model with this γ\n",
    "    m = VLDecisionsPackage.build(MyMDPProblemModel, (𝒮 = 𝒮, 𝒜 = 𝒜, T = T, R = R, γ = γ));\n",
    "    \n",
    "    for j ∈ eachindex(k_array)\n",
    "        k_max = k_array[j];\n",
    "        \n",
    "        # compute\n",
    "        value_iteration_model = MyValueIterationModel(k_max);\n",
    "        solution = VLDecisionsPackage.solve(value_iteration_model, m);\n",
    "        my_Q = Q(m, solution.U)\n",
    "        my_π = policy(my_Q);\n",
    "        \n",
    "        # store -\n",
    "        policy_dictionary[(γ,k_max)] = my_π;\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc3e5af1-6506-43dc-b57c-61b3b23e29ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize ...\n",
    "policy_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca330c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "move_arrows = Dict{Int,Any}();\n",
    "move_arrows[1] = \"←\"\n",
    "move_arrows[2] = \"→\"\n",
    "move_arrows[3] = \"↓\"\n",
    "move_arrows[4] = \"↑\"\n",
    "move_arrows[5] = \"∅\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe1a2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_π = policy_dictionary[(0.1,10)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c39201d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s ∈ 𝒮\n",
    "    \n",
    "    a = my_π[s];\n",
    "    Δ = world.moves[a];\n",
    "    current_position = world.coordinates[s]\n",
    "    new_position =  current_position .+ Δ\n",
    "    \n",
    "    if (in(current_position, absorbing_state_set) == true)\n",
    "        println(\"$(current_position) $(move_arrows[5])\")\n",
    "    else\n",
    "        println(\"$(current_position) $(move_arrows[a]) $(new_position)\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a428e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# setup -\n",
    "# draw the path -\n",
    "p = plot();\n",
    "initial_site = (1,1)\n",
    "hit_absorbing_state = false\n",
    "s = world.states[initial_site];\n",
    "visited_sites = Set{Tuple{Int,Int}}();\n",
    "push!(visited_sites, initial_site);\n",
    "\n",
    "while (hit_absorbing_state == false)\n",
    "    current_position = world.coordinates[s]\n",
    "    a = my_π[s];\n",
    "    Δ = world.moves[a];\n",
    "    new_position =  current_position .+ Δ\n",
    "    scatter!([current_position[1]],[current_position[2]], label=\"\", showaxis=:false, msc=:black, c=:blue)\n",
    "    plot!([current_position[1], new_position[1]],[current_position[2],new_position[2]], label=\"\", arrow=true, lw=2, c=:gray)\n",
    "    \n",
    "    if (in(new_position, absorbing_state_set) == true || in(new_position, visited_sites) == true)\n",
    "        hit_absorbing_state = true;\n",
    "    else\n",
    "        s = world.states[new_position];\n",
    "        push!(visited_sites, new_position);\n",
    "    end\n",
    "end\n",
    "\n",
    "# draw the grid -\n",
    "for s ∈ 𝒮\n",
    "    current_position = world.coordinates[s]\n",
    "    a = my_π[s];\n",
    "    Δ = world.moves[a];\n",
    "    new_position =  current_position .+ Δ\n",
    "    \n",
    "    if (haskey(rewards, current_position) == true && rewards[current_position] == charging_reward)\n",
    "        scatter!([current_position[1]],[current_position[2]], label=\"\", showaxis=:false, c=:green, ms=6)\n",
    "    elseif (haskey(rewards, current_position) == true && rewards[current_position] == lava_reward)\n",
    "        scatter!([current_position[1]],[current_position[2]], label=\"\", showaxis=:false, c=:red, ms=6)\n",
    "    elseif (in(current_position, soft_wall_set) == true)\n",
    "        scatter!([current_position[1]],[current_position[2]], label=\"\", showaxis=:false, c=:gray69, ms=6)\n",
    "    else\n",
    "        scatter!([current_position[1]],[current_position[2]], label=\"\", showaxis=:false, msc=:black, c=:white)\n",
    "    end\n",
    "end\n",
    "current()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c1534c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
